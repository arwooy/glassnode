"""
GlassnodeæŒ‡æ ‡ä¿¡æ¯å¢ç›Š(Information Gain)åˆ†æ
è®¡ç®—å„æŒ‡æ ‡å¯¹æœªæ¥ä»·æ ¼é¢„æµ‹çš„ä¿¡æ¯å¢ç›Šï¼Œè¯„ä¼°é¢„æµ‹ä»·å€¼
"""

import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import json
import time
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import entropy
from sklearn.feature_selection import mutual_info_regression, mutual_info_classif
from sklearn.preprocessing import KBinsDiscretizer
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False


class InformationGainAnalyzer:
    """ä¿¡æ¯å¢ç›Šåˆ†æå™¨"""
    
    def __init__(self):
        self.results = {}
        
    def calculate_entropy(self, data: np.ndarray, bins: int = 10) -> float:
        """
        è®¡ç®—æ•°æ®çš„ç†µ
        
        Parameters:
        - data: æ•°æ®æ•°ç»„
        - bins: åˆ†ç®±æ•°é‡
        
        Returns:
        - ç†µå€¼
        """
        # ç¦»æ•£åŒ–æ•°æ®
        hist, _ = np.histogram(data[~np.isnan(data)], bins=bins)
        # è®¡ç®—æ¦‚ç‡
        probs = hist / hist.sum()
        # ç§»é™¤é›¶æ¦‚ç‡
        probs = probs[probs > 0]
        # è®¡ç®—ç†µ
        return -np.sum(probs * np.log2(probs))
    
    def calculate_conditional_entropy(self, X: np.ndarray, Y: np.ndarray, bins: int = 10) -> float:
        """
        è®¡ç®—æ¡ä»¶ç†µ H(Y|X)
        
        Parameters:
        - X: æ¡ä»¶å˜é‡
        - Y: ç›®æ ‡å˜é‡
        - bins: åˆ†ç®±æ•°é‡
        
        Returns:
        - æ¡ä»¶ç†µ
        """
        # ç¦»æ•£åŒ–æ•°æ®
        X_discrete = pd.qcut(X, bins, labels=False, duplicates='drop')
        Y_discrete = pd.qcut(Y, bins, labels=False, duplicates='drop')
        
        # è®¡ç®—è”åˆæ¦‚ç‡å’Œè¾¹é™…æ¦‚ç‡
        joint_prob = pd.crosstab(X_discrete, Y_discrete, normalize=True)
        X_prob = pd.Series(X_discrete).value_counts(normalize=True)
        
        # è®¡ç®—æ¡ä»¶ç†µ
        conditional_entropy = 0
        for x_val in X_prob.index:
            if x_val in joint_prob.index:
                # P(Y|X=x)çš„åˆ†å¸ƒ
                conditional_dist = joint_prob.loc[x_val] / X_prob[x_val]
                # ç§»é™¤é›¶æ¦‚ç‡
                conditional_dist = conditional_dist[conditional_dist > 0]
                # è®¡ç®—è¯¥æ¡ä»¶ä¸‹çš„ç†µ
                h = -np.sum(conditional_dist * np.log2(conditional_dist))
                # åŠ æƒç´¯åŠ 
                conditional_entropy += X_prob[x_val] * h
        
        return conditional_entropy
    
    def calculate_information_gain(self, indicator: np.ndarray, target: np.ndarray, bins: int = 10) -> Dict:
        """
        è®¡ç®—ä¿¡æ¯å¢ç›Š
        
        Parameters:
        - indicator: æŒ‡æ ‡æ•°æ®
        - target: ç›®æ ‡æ•°æ®ï¼ˆæœªæ¥ä»·æ ¼å˜åŒ–ï¼‰
        - bins: åˆ†ç®±æ•°é‡
        
        Returns:
        - ä¿¡æ¯å¢ç›Šç›¸å…³æŒ‡æ ‡
        """
        # ç§»é™¤NaN
        valid_mask = ~(np.isnan(indicator) | np.isnan(target))
        indicator_clean = indicator[valid_mask]
        target_clean = target[valid_mask]
        
        if len(indicator_clean) < 100:
            return {}
        
        # ç¦»æ•£åŒ–æ•°æ®
        try:
            # ä½¿ç”¨åˆ†ä½æ•°è¿›è¡Œç¦»æ•£åŒ–
            indicator_discrete = pd.qcut(indicator_clean, bins, labels=False, duplicates='drop')
            target_discrete = pd.qcut(target_clean, bins, labels=False, duplicates='drop')
        except:
            # å¦‚æœåˆ†ä½æ•°å¤±è´¥ï¼Œä½¿ç”¨ç­‰å®½åˆ†ç®±
            indicator_discrete = pd.cut(indicator_clean, bins, labels=False)
            target_discrete = pd.cut(target_clean, bins, labels=False)
        
        # è®¡ç®—ç›®æ ‡å˜é‡çš„ç†µ
        target_probs = pd.Series(target_discrete).value_counts(normalize=True)
        H_target = -np.sum(target_probs * np.log2(target_probs + 1e-10))
        
        # è®¡ç®—æ¡ä»¶ç†µ H(Y|X)
        H_target_given_indicator = 0
        indicator_probs = pd.Series(indicator_discrete).value_counts(normalize=True)
        
        for x_val in indicator_probs.index:
            # è·å–X=x_valæ—¶çš„Yåˆ†å¸ƒ
            mask = indicator_discrete == x_val
            if mask.sum() > 0:
                y_given_x = target_discrete[mask]
                y_probs = pd.Series(y_given_x).value_counts(normalize=True)
                # è®¡ç®—H(Y|X=x_val)
                h_y_given_x = -np.sum(y_probs * np.log2(y_probs + 1e-10))
                # åŠ æƒ
                H_target_given_indicator += indicator_probs[x_val] * h_y_given_x
        
        # ä¿¡æ¯å¢ç›Šï¼ˆå¿…é¡»ä¸ºéè´Ÿï¼‰
        information_gain = max(0, H_target - H_target_given_indicator)
        
        # æŒ‡æ ‡ç†µ
        H_indicator = -np.sum(indicator_probs * np.log2(indicator_probs + 1e-10))
        
        # ä¿¡æ¯å¢ç›Šæ¯”ï¼ˆå½’ä¸€åŒ–ï¼‰
        gain_ratio = information_gain / H_indicator if H_indicator > 0 else 0
        
        # å¯¹ç§°ä¸ç¡®å®šæ€§ï¼ˆSymmetric Uncertaintyï¼‰
        symmetric_uncertainty = 2 * information_gain / (H_target + H_indicator) if (H_target + H_indicator) > 0 else 0
        
        # è®¡ç®—äº’ä¿¡æ¯ï¼ˆå¦ä¸€ç§æ–¹å¼ï¼‰
        from sklearn.metrics import mutual_info_score
        mutual_info = mutual_info_score(indicator_discrete, target_discrete)
        
        return {
            'information_gain': information_gain,
            'gain_ratio': gain_ratio,
            'symmetric_uncertainty': symmetric_uncertainty,
            'mutual_information_discrete': mutual_info,
            'normalized_mi_discrete': mutual_info / min(H_target, H_indicator) if min(H_target, H_indicator) > 0 else 0,
            'target_entropy': H_target,
            'conditional_entropy': H_target_given_indicator,
            'indicator_entropy': H_indicator,
            'reduction_ratio': information_gain / H_target if H_target > 0 else 0  # ä¸ç¡®å®šæ€§å‡å°‘æ¯”ä¾‹
        }
    
    def calculate_mutual_information(self, indicator: pd.Series, target: pd.Series, 
                                   discrete: bool = False) -> Dict:
        """
        ä½¿ç”¨sklearnè®¡ç®—äº’ä¿¡æ¯
        
        Parameters:
        - indicator: æŒ‡æ ‡æ•°æ®
        - target: ç›®æ ‡æ•°æ®
        - discrete: æ˜¯å¦ç¦»æ•£åŒ–
        
        Returns:
        - äº’ä¿¡æ¯ç›¸å…³æŒ‡æ ‡
        """
        # å‡†å¤‡æ•°æ®
        data = pd.DataFrame({
            'indicator': indicator,
            'target': target
        }).dropna()
        
        if len(data) < 100:
            return {}
        
        X = data[['indicator']].values
        y = data['target'].values
        
        if discrete:
            # ç¦»æ•£åŒ–ç›®æ ‡å˜é‡ï¼ˆåˆ†ç±»ï¼‰
            discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
            y_discrete = discretizer.fit_transform(y.reshape(-1, 1)).ravel().astype(int)
            
            # è®¡ç®—åˆ†ç±»äº’ä¿¡æ¯
            mi_score = mutual_info_classif(X, y_discrete, random_state=42)[0]
            
            # è®¡ç®—æœ€å¤§å¯èƒ½çš„äº’ä¿¡æ¯ï¼ˆå®Œç¾é¢„æµ‹ï¼‰
            max_mi = self.calculate_entropy(y_discrete, bins=5)
            
        else:
            # è®¡ç®—å›å½’äº’ä¿¡æ¯
            mi_score = mutual_info_regression(X, y, random_state=42)[0]
            
            # ä¼°è®¡æœ€å¤§äº’ä¿¡æ¯ï¼ˆä½¿ç”¨ç›®æ ‡å˜é‡çš„ç†µä½œä¸ºä¸Šç•Œï¼‰
            max_mi = self.calculate_entropy(y, bins=10)
        
        # å½’ä¸€åŒ–äº’ä¿¡æ¯
        normalized_mi = mi_score / max_mi if max_mi > 0 else 0
        
        return {
            'mutual_information': mi_score,
            'normalized_mi': normalized_mi,
            'max_possible_mi': max_mi
        }
    
    def calculate_transfer_entropy(self, indicator: pd.Series, target: pd.Series, 
                                  lag: int = 1, bins: int = 5) -> float:
        """
        è®¡ç®—è½¬ç§»ç†µï¼ˆTransfer Entropyï¼‰
        è¡¡é‡æŒ‡æ ‡å¯¹ç›®æ ‡çš„ä¿¡æ¯æµåŠ¨
        
        Parameters:
        - indicator: æŒ‡æ ‡æ•°æ®
        - target: ç›®æ ‡æ•°æ®  
        - lag: æ»åæœŸ
        - bins: åˆ†ç®±æ•°é‡
        
        Returns:
        - è½¬ç§»ç†µå€¼
        """
        # å‡†å¤‡æ•°æ®
        n = len(target)
        if n < lag + 100:
            return 0
        
        # åˆ›å»ºæ»åå˜é‡
        target_current = target[lag:].values
        target_past = target[:-lag].values
        indicator_past = indicator[:-lag].values
        
        # ç¦»æ•£åŒ–
        discretizer = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='quantile')
        
        # å®‰å…¨åœ°è¿›è¡Œç¦»æ•£åŒ–
        try:
            target_current_d = discretizer.fit_transform(target_current.reshape(-1, 1)).ravel()
            target_past_d = discretizer.fit_transform(target_past.reshape(-1, 1)).ravel()
            indicator_past_d = discretizer.fit_transform(indicator_past.reshape(-1, 1)).ravel()
        except:
            return 0
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        # P(target_t | target_t-1, indicator_t-1)
        joint_with_indicator = pd.crosstab(
            [target_past_d, indicator_past_d],
            target_current_d,
            normalize=True
        )
        
        # P(target_t | target_t-1)
        joint_without_indicator = pd.crosstab(
            target_past_d,
            target_current_d,
            normalize=True
        )
        
        # è®¡ç®—è½¬ç§»ç†µ
        te = 0
        for tp in np.unique(target_past_d):
            for ip in np.unique(indicator_past_d):
                for tc in np.unique(target_current_d):
                    # è”åˆæ¦‚ç‡
                    if (tp, ip) in joint_with_indicator.index and tc in joint_with_indicator.columns:
                        p_joint = joint_with_indicator.loc[(tp, ip), tc]
                    else:
                        p_joint = 0
                    
                    if p_joint > 0:
                        # æ¡ä»¶æ¦‚ç‡
                        if tp in joint_without_indicator.index and tc in joint_without_indicator.columns:
                            p_cond_without = joint_without_indicator.loc[tp, tc]
                        else:
                            p_cond_without = 0
                        
                        if p_cond_without > 0:
                            # è¾¹é™…æ¦‚ç‡
                            p_marginal = (target_past_d == tp).mean() * (indicator_past_d == ip).mean()
                            
                            if p_marginal > 0:
                                # è½¬ç§»ç†µè´¡çŒ®
                                te += p_joint * np.log2(p_joint / (p_cond_without * p_marginal))
        
        return te
    
    def analyze_predictive_information(self, indicator_df: pd.DataFrame, price_df: pd.DataFrame,
                                      horizons: List[int] = [1, 3, 7, 14, 30]) -> Dict:
        """
        åˆ†ææŒ‡æ ‡çš„é¢„æµ‹ä¿¡æ¯å«é‡
        
        Parameters:
        - indicator_df: æŒ‡æ ‡æ•°æ®
        - price_df: ä»·æ ¼æ•°æ®
        - horizons: é¢„æµ‹æ—¶é—´è·¨åº¦åˆ—è¡¨
        
        Returns:
        - å„æ—¶é—´è·¨åº¦çš„ä¿¡æ¯å¢ç›ŠæŒ‡æ ‡
        """
        results = {}
        
        # åˆå¹¶æ•°æ®
        data = pd.merge(indicator_df, price_df, left_index=True, right_index=True, how='inner')
        indicator_col = data.iloc[:, 0]
        price_col = data.iloc[:, -1]
        
        for horizon in horizons:
            # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
            future_return = price_col.pct_change(horizon).shift(-horizon)
            
            # ä¿¡æ¯å¢ç›Šåˆ†æï¼ˆè¿ç»­å˜é‡ï¼‰
            ig_continuous = self.calculate_information_gain(
                indicator_col.values,
                future_return.values,
                bins=10
            )
            
            # äº’ä¿¡æ¯åˆ†æï¼ˆå›å½’ï¼‰
            mi_regression = self.calculate_mutual_information(
                indicator_col,
                future_return,
                discrete=False
            )
            
            # äº’ä¿¡æ¯åˆ†æï¼ˆåˆ†ç±»ï¼‰
            mi_classification = self.calculate_mutual_information(
                indicator_col,
                future_return,
                discrete=True
            )
            
            # è½¬ç§»ç†µ
            te_score = self.calculate_transfer_entropy(
                indicator_col,
                future_return,
                lag=horizon,
                bins=5
            )
            
            # è®¡ç®—ç›¸å…³ç³»æ•°ä½œä¸ºå¯¹æ¯”
            correlation = indicator_col.corr(future_return)
            
            results[horizon] = {
                'information_gain': ig_continuous.get('information_gain', 0),
                'gain_ratio': ig_continuous.get('gain_ratio', 0),
                'symmetric_uncertainty': ig_continuous.get('symmetric_uncertainty', 0),
                'reduction_ratio': ig_continuous.get('reduction_ratio', 0),
                'mutual_info_regression': mi_regression.get('mutual_information', 0),
                'normalized_mi_regression': mi_regression.get('normalized_mi', 0),
                'mutual_info_classification': mi_classification.get('mutual_information', 0),
                'normalized_mi_classification': mi_classification.get('normalized_mi', 0),
                'transfer_entropy': te_score,
                'correlation': correlation,
                'correlation_squared': correlation ** 2  # R-squared
            }
        
        return results


class IGVisualization:
    """ä¿¡æ¯å¢ç›Šå¯è§†åŒ–"""
    
    @staticmethod
    def plot_ig_comparison(all_results: Dict, metric: str = 'information_gain'):
        """
        ç»˜åˆ¶ä¿¡æ¯å¢ç›Šå¯¹æ¯”å›¾
        
        Parameters:
        - all_results: æ‰€æœ‰æŒ‡æ ‡çš„IGç»“æœ
        - metric: è¦æ˜¾ç¤ºçš„æŒ‡æ ‡
        """
        # å‡†å¤‡æ•°æ®
        data = []
        for indicator, horizons in all_results.items():
            for horizon, metrics in horizons.items():
                if metric in metrics:
                    data.append({
                        'indicator': indicator.replace('_', ' ').title(),
                        'horizon': horizon,
                        metric: metrics[metric]
                    })
        
        if not data:
            return
        
        df = pd.DataFrame(data)
        
        # åˆ›å»ºé€è§†è¡¨
        pivot = df.pivot(index='indicator', columns='horizon', values=metric)
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        plt.figure(figsize=(14, 10))
        
        # ä½¿ç”¨é€‚å½“çš„é¢œè‰²æ˜ å°„
        if 'correlation' in metric:
            cmap = 'RdBu_r'
            center = 0
            vmin, vmax = -1, 1
        else:
            cmap = 'YlOrRd'
            center = None
            vmin, vmax = 0, pivot.max().max()
        
        sns.heatmap(
            pivot,
            annot=True,
            fmt='.3f',
            cmap=cmap,
            center=center,
            vmin=vmin,
            vmax=vmax,
            cbar_kws={'label': metric.replace('_', ' ').title()},
            linewidths=0.5
        )
        
        plt.title(f'{metric.replace("_", " ").title()} - å„æŒ‡æ ‡ä¸åŒé¢„æµ‹æœŸçš„ä¿¡æ¯å«é‡')
        plt.xlabel('é¢„æµ‹æ—¶é—´è·¨åº¦ï¼ˆå¤©ï¼‰')
        plt.ylabel('æŒ‡æ ‡')
        plt.tight_layout()
        plt.savefig(f'ig_{metric}_heatmap.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    @staticmethod
    def plot_ig_by_horizon(all_results: Dict):
        """
        æŒ‰æ—¶é—´è·¨åº¦ç»˜åˆ¶ä¿¡æ¯å¢ç›Š
        """
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        horizons = [1, 3, 7, 14, 30]
        
        for idx, horizon in enumerate(horizons):
            ax = axes[idx // 3, idx % 3]
            
            # æ”¶é›†è¯¥æ—¶é—´è·¨åº¦çš„æ•°æ®
            ig_scores = []
            mi_scores = []
            indicators = []
            
            for indicator, h_data in all_results.items():
                if horizon in h_data:
                    indicators.append(indicator.split('_')[-1])  # ç®€åŒ–åç§°
                    ig_scores.append(h_data[horizon].get('information_gain', 0))
                    mi_scores.append(h_data[horizon].get('normalized_mi_regression', 0))
            
            if indicators:
                x = np.arange(len(indicators))
                width = 0.35
                
                bars1 = ax.bar(x - width/2, ig_scores, width, label='Information Gain', color='steelblue')
                bars2 = ax.bar(x + width/2, mi_scores, width, label='Normalized MI', color='coral')
                
                ax.set_xlabel('æŒ‡æ ‡')
                ax.set_ylabel('ä¿¡æ¯é‡')
                ax.set_title(f'{horizon}å¤©é¢„æµ‹')
                ax.set_xticks(x)
                ax.set_xticklabels(indicators, rotation=45, ha='right')
                ax.legend()
                ax.grid(True, alpha=0.3)
        
        # éšè—å¤šä½™çš„å­å›¾
        if len(horizons) < 6:
            axes[-1, -1].axis('off')
        
        plt.suptitle('ä¸åŒæ—¶é—´è·¨åº¦çš„ä¿¡æ¯å¢ç›Šå¯¹æ¯”', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('ig_by_horizon.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    @staticmethod
    def plot_ig_ranking(all_results: Dict):
        """
        ç»˜åˆ¶ä¿¡æ¯å¢ç›Šæ’å
        """
        # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å¹³å‡ä¿¡æ¯å¢ç›Š
        avg_scores = {}
        
        for indicator, horizons in all_results.items():
            scores = []
            for h, metrics in horizons.items():
                # ç»¼åˆå¤šä¸ªä¿¡æ¯æŒ‡æ ‡
                composite_score = (
                    metrics.get('information_gain', 0) * 0.3 +
                    metrics.get('normalized_mi_regression', 0) * 0.3 +
                    metrics.get('symmetric_uncertainty', 0) * 0.2 +
                    metrics.get('transfer_entropy', 0) * 0.2
                )
                scores.append(composite_score)
            
            avg_scores[indicator] = np.mean(scores)
        
        # æ’åº
        sorted_indicators = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ç»˜å›¾
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # æ¡å½¢å›¾
        indicators = [x[0].replace('_', ' ').title() for x in sorted_indicators[:10]]
        scores = [x[1] for x in sorted_indicators[:10]]
        
        ax1.barh(range(len(indicators)), scores, color='teal')
        ax1.set_yticks(range(len(indicators)))
        ax1.set_yticklabels(indicators)
        ax1.set_xlabel('ç»¼åˆä¿¡æ¯å¾—åˆ†')
        ax1.set_title('Top 10 é¢„æµ‹ä¿¡æ¯å«é‡æŒ‡æ ‡')
        ax1.invert_yaxis()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(scores):
            ax1.text(v + 0.001, i, f'{v:.3f}', va='center')
        
        # é›·è¾¾å›¾
        categories = ['IG', 'MI', 'SU', 'TE', 'Corr']
        
        # é€‰æ‹©å‰5ä¸ªæŒ‡æ ‡
        top_5 = sorted_indicators[:5]
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]
        
        ax2 = plt.subplot(122, projection='polar')
        
        for indicator, _ in top_5:
            if indicator in all_results:
                # å–7å¤©é¢„æµ‹çš„æ•°æ®
                if 7 in all_results[indicator]:
                    metrics = all_results[indicator][7]
                    values = [
                        metrics.get('information_gain', 0),
                        metrics.get('normalized_mi_regression', 0),
                        metrics.get('symmetric_uncertainty', 0),
                        metrics.get('transfer_entropy', 0),
                        abs(metrics.get('correlation', 0))
                    ]
                    values += values[:1]
                    
                    ax2.plot(angles, values, 'o-', linewidth=2, 
                            label=indicator.split('_')[-1])
                    ax2.fill(angles, values, alpha=0.1)
        
        ax2.set_xticks(angles[:-1])
        ax2.set_xticklabels(categories)
        ax2.set_title('ä¿¡æ¯æŒ‡æ ‡é›·è¾¾å›¾ï¼ˆ7å¤©é¢„æµ‹ï¼‰')
        ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('ig_ranking.png', dpi=150, bbox_inches='tight')
        plt.show()


class IGReportGenerator:
    """ä¿¡æ¯å¢ç›ŠæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    @staticmethod
    def generate_summary(all_results: Dict) -> Dict:
        """
        ç”Ÿæˆä¿¡æ¯å¢ç›Šåˆ†ææ‘˜è¦
        """
        summary = {
            'best_indicators_by_horizon': {},
            'best_indicators_by_metric': {},
            'average_scores': {},
            'insights': []
        }
        
        # æŒ‰æ—¶é—´è·¨åº¦æ‰¾æœ€ä½³æŒ‡æ ‡
        for horizon in [1, 3, 7, 14, 30]:
            best_ig = None
            best_mi = None
            best_te = None
            
            for indicator, h_data in all_results.items():
                if horizon in h_data:
                    metrics = h_data[horizon]
                    
                    # Information Gain
                    if best_ig is None or metrics.get('information_gain', 0) > best_ig[1]:
                        best_ig = (indicator, metrics.get('information_gain', 0))
                    
                    # Mutual Information
                    if best_mi is None or metrics.get('normalized_mi_regression', 0) > best_mi[1]:
                        best_mi = (indicator, metrics.get('normalized_mi_regression', 0))
                    
                    # Transfer Entropy
                    if best_te is None or metrics.get('transfer_entropy', 0) > best_te[1]:
                        best_te = (indicator, metrics.get('transfer_entropy', 0))
            
            summary['best_indicators_by_horizon'][horizon] = {
                'information_gain': best_ig,
                'mutual_information': best_mi,
                'transfer_entropy': best_te
            }
        
        # è®¡ç®—å¹³å‡å¾—åˆ†
        for indicator, h_data in all_results.items():
            avg_ig = np.mean([m.get('information_gain', 0) for m in h_data.values()])
            avg_mi = np.mean([m.get('normalized_mi_regression', 0) for m in h_data.values()])
            avg_te = np.mean([m.get('transfer_entropy', 0) for m in h_data.values()])
            
            summary['average_scores'][indicator] = {
                'avg_information_gain': avg_ig,
                'avg_mutual_information': avg_mi,
                'avg_transfer_entropy': avg_te,
                'composite_score': (avg_ig + avg_mi + avg_te) / 3
            }
        
        # ç”Ÿæˆæ´å¯Ÿ
        # æ‰¾å‡ºä¿¡æ¯å«é‡æœ€é«˜çš„æŒ‡æ ‡
        best_overall = max(summary['average_scores'].items(), 
                         key=lambda x: x[1]['composite_score'])
        summary['insights'].append(
            f"æœ€é«˜ä¿¡æ¯å«é‡æŒ‡æ ‡: {best_overall[0]} (ç»¼åˆå¾—åˆ†: {best_overall[1]['composite_score']:.3f})"
        )
        
        # æ‰¾å‡ºæœ€ä½³é¢„æµ‹æ—¶é—´è·¨åº¦
        horizon_scores = {}
        for h in [1, 3, 7, 14, 30]:
            scores = []
            for ind, h_data in all_results.items():
                if h in h_data:
                    scores.append(h_data[h].get('information_gain', 0))
            horizon_scores[h] = np.mean(scores) if scores else 0
        
        best_horizon = max(horizon_scores.items(), key=lambda x: x[1])
        summary['insights'].append(
            f"æœ€ä½³é¢„æµ‹æ—¶é—´è·¨åº¦: {best_horizon[0]}å¤© (å¹³å‡IG: {best_horizon[1]:.3f})"
        )
        
        return summary


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    API_KEY = "myapi_sk_b3fa36048ea022be1c21e626742d4dec"
    START_DATE = "2021-01-01"
    END_DATE = "2024-12-31"
    
    # è¦åˆ†æçš„æŒ‡æ ‡
    METRICS_TO_ANALYZE = [
        ('market', 'price_usd_close'),
        ('market', 'mvrv'),
        ('market', 'mvrv_z_score'),
        ('indicators', 'sopr'),
        ('indicators', 'net_unrealized_profit_loss'),
        ('indicators', 'puell_multiple'),
        ('indicators', 'reserve_risk'),
        ('supply', 'profit_relative'),
        ('addresses', 'active_count'),
        ('mining', 'hash_rate_mean')
    ]
    
    HORIZONS = [1, 3, 7, 14, 30]
    
    print("=" * 80)
    print("ğŸ“Š GlassnodeæŒ‡æ ‡ä¿¡æ¯å¢ç›Š(Information Gain)åˆ†æ")
    print("=" * 80)
    print(f"åˆ†æå‘¨æœŸ: {START_DATE} è‡³ {END_DATE}")
    print(f"é¢„æµ‹è·¨åº¦: {HORIZONS} å¤©")
    
    # åˆå§‹åŒ–
    ig_analyzer = InformationGainAnalyzer()
    visualizer = IGVisualization()
    reporter = IGReportGenerator()
    
    # è·å–æ•°æ®ï¼ˆä½¿ç”¨ä¹‹å‰çš„æ•°æ®è·å–é€»è¾‘ï¼‰
    print("\nğŸ“Š Step 1: è·å–æ•°æ®...")
    
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    # å®é™…ä½¿ç”¨æ—¶åº”è¯¥è°ƒç”¨Glassnode API
    from glassnode_prediction_analysis import GlassnodeDataFetcher
    
    fetcher = GlassnodeDataFetcher(API_KEY)
    metrics_data = fetcher.fetch_metrics(METRICS_TO_ANALYZE, START_DATE, END_DATE)
    
    if 'market_price_usd_close' not in metrics_data:
        print("âŒ æ— æ³•è·å–ä»·æ ¼æ•°æ®")
        return
    
    price_df = metrics_data['market_price_usd_close'].rename(columns={'price_usd_close': 'price'})
    
    # åˆ†æä¿¡æ¯å¢ç›Š
    print("\nğŸ“Š Step 2: è®¡ç®—ä¿¡æ¯å¢ç›Š...")
    all_results = {}
    
    for metric_name, metric_df in metrics_data.items():
        if metric_name == 'market_price_usd_close':
            continue
        
        print(f"\nåˆ†æ {metric_name}...")
        results = ig_analyzer.analyze_predictive_information(
            metric_df, price_df, HORIZONS
        )
        
        all_results[metric_name] = results
        
        # æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
        for h in [1, 7, 30]:
            if h in results:
                ig = results[h].get('information_gain', 0)
                mi = results[h].get('normalized_mi_regression', 0)
                print(f"  {h}å¤©: IG={ig:.3f}, MI={mi:.3f}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ“Š Step 3: ç”Ÿæˆå¯è§†åŒ–...")
    
    # ä¿¡æ¯å¢ç›Šçƒ­åŠ›å›¾
    visualizer.plot_ig_comparison(all_results, 'information_gain')
    print("  âœ… ä¿¡æ¯å¢ç›Šçƒ­åŠ›å›¾")
    
    # äº’ä¿¡æ¯çƒ­åŠ›å›¾
    visualizer.plot_ig_comparison(all_results, 'normalized_mi_regression')
    print("  âœ… äº’ä¿¡æ¯çƒ­åŠ›å›¾")
    
    # å¯¹ç§°ä¸ç¡®å®šæ€§çƒ­åŠ›å›¾
    visualizer.plot_ig_comparison(all_results, 'symmetric_uncertainty')
    print("  âœ… å¯¹ç§°ä¸ç¡®å®šæ€§çƒ­åŠ›å›¾")
    
    # æŒ‰æ—¶é—´è·¨åº¦å¯¹æ¯”
    visualizer.plot_ig_by_horizon(all_results)
    print("  âœ… æ—¶é—´è·¨åº¦å¯¹æ¯”å›¾")
    
    # ç»¼åˆæ’å
    visualizer.plot_ig_ranking(all_results)
    print("  âœ… ç»¼åˆæ’åå›¾")
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š Step 4: ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
    summary = reporter.generate_summary(all_results)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_report = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'period': {'start': START_DATE, 'end': END_DATE},
        'summary': summary,
        'detailed_results': {
            indicator: {
                str(horizon): metrics
                for horizon, metrics in horizons.items()
            }
            for indicator, horizons in all_results.items()
        }
    }
    
    with open('information_gain_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(detailed_report, f, indent=2, ensure_ascii=False, default=str)
    
    print("âœ… æŠ¥å‘Šå·²ä¿å­˜: information_gain_analysis.json")
    
    # æ‰“å°å…³é”®å‘ç°
    print("\n" + "=" * 80)
    print("ğŸ¯ å…³é”®å‘ç°")
    print("=" * 80)
    
    print("\nğŸ“ˆ æœ€ä½³ä¿¡æ¯å¢ç›ŠæŒ‡æ ‡ï¼ˆæŒ‰æ—¶é—´è·¨åº¦ï¼‰:")
    for horizon, best in summary['best_indicators_by_horizon'].items():
        if best['information_gain']:
            print(f"  {horizon}å¤©: {best['information_gain'][0]} (IG={best['information_gain'][1]:.3f})")
    
    print("\nğŸ“Š ç»¼åˆä¿¡æ¯å«é‡æ’åï¼ˆå‰5ï¼‰:")
    sorted_avg = sorted(summary['average_scores'].items(), 
                       key=lambda x: x[1]['composite_score'], 
                       reverse=True)[:5]
    
    for i, (indicator, scores) in enumerate(sorted_avg, 1):
        print(f"  {i}. {indicator}:")
        print(f"     - ä¿¡æ¯å¢ç›Š: {scores['avg_information_gain']:.3f}")
        print(f"     - äº’ä¿¡æ¯: {scores['avg_mutual_information']:.3f}")
        print(f"     - è½¬ç§»ç†µ: {scores['avg_transfer_entropy']:.3f}")
        print(f"     - ç»¼åˆå¾—åˆ†: {scores['composite_score']:.3f}")
    
    print("\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
    for insight in summary['insights']:
        print(f"  â€¢ {insight}")
    
    # åˆ›å»ºIGä¸ç›¸å…³æ€§å¯¹æ¯”è¡¨
    print("\nğŸ“‹ ä¿¡æ¯å¢ç›Š vs ç›¸å…³æ€§å¯¹æ¯”:")
    print("=" * 80)
    print(f"{'æŒ‡æ ‡':<30} {'7å¤©IG':>10} {'7å¤©MI':>10} {'7å¤©ç›¸å…³æ€§':>10} {'IG/Corræ¯”':>10}")
    print("-" * 80)
    
    for indicator, h_data in all_results.items():
        if 7 in h_data:
            ig = h_data[7].get('information_gain', 0)
            mi = h_data[7].get('normalized_mi_regression', 0)
            corr = abs(h_data[7].get('correlation', 0))
            ratio = ig / corr if corr > 0 else 0
            
            ind_name = indicator.split('_')[-1][:25]
            print(f"{ind_name:<30} {ig:>10.3f} {mi:>10.3f} {corr:>10.3f} {ratio:>10.2f}")
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  1. information_gain_analysis.json - è¯¦ç»†åˆ†æç»“æœ")
    print("  2. ig_information_gain_heatmap.png - ä¿¡æ¯å¢ç›Šçƒ­åŠ›å›¾")
    print("  3. ig_normalized_mi_regression_heatmap.png - äº’ä¿¡æ¯çƒ­åŠ›å›¾")
    print("  4. ig_symmetric_uncertainty_heatmap.png - å¯¹ç§°ä¸ç¡®å®šæ€§çƒ­åŠ›å›¾")
    print("  5. ig_by_horizon.png - æ—¶é—´è·¨åº¦å¯¹æ¯”")
    print("  6. ig_ranking.png - ç»¼åˆæ’åå›¾")


if __name__ == "__main__":
    main()